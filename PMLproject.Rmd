---
title: "Practical Machine Learning Course Project"
author: "Tiffany Stoeke"
date: "November 12, 2015"
output: html_document
---

###Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity.  For this project, we use the Weight Lifting Exercises data set found at http://groupware.les.inf.puc-rio.br/har.  In this data set, six participants wore accelerometers and were asked to perform barbell lifts correctly and incorrectly in 5 different ways, with Class A being the correct method and Classes B-E using an incorrect method. Our goal is to use data from the Weight Lifting Exercises dataset to predict the manner in which the participants performed the exercise.

###Data Transformation

To begin, download the testing and training data sets and load required packages.  We will set the testing dataset aside for now as it will be used for final validation.

```{r, cache=TRUE}
training<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                   na.strings=c("NA","#DIV/0!", ""))
testing<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  na.strings=c("NA","#DIV/0!", ""))
library(AppliedPredictiveModeling)
library(caret)
library(randomForest)
library(rattle)
str(training)
```

A review of the structure of the training set reveals a large number of missing values, which we remove.  We also remove the first seven columns of the data set as not relevant to our review since the person performing the maneuver or the time it was performed should not impact the class to which the activity belongs.  Finally, we divide the cleaned training set into two new datasets for cross-validation purposes.  We split the data based on the "classe" variable and create a new training set using 60% of the original data, with the remaining 40% of the data split onto a new testing set.

```{r, cache=TRUE}
trainingnoNA<-training[,colSums(is.na(training))==0]
testingnoNA<-testing[,colSums(is.na(testing))==0]

trainingnoNA<-trainingnoNA[,-c(1:7)]
testingnoNA<-testingnoNA[,-c(1:7)]

inTrain<-createDataPartition(y=trainingnoNA$classe,p=0.6,list=FALSE)
newtrain<-trainingnoNA[inTrain,]
newtest<-trainingnoNA[-inTrain,]
```

Before beginning our modeling, let's check the frequency of the appearance of Classes A-E.  If there is severe imbalance, weighting may need to be considered.  It would also be difficult to make an accurate model if one of the classes has little to no appearance in our new training data set.

```{r, cache=TRUE}
plot(newtrain$classe,main="Frequency of 'Classe' Variable",xlab="'Classe' Variable",ylab="Frequency",col="darkorchid")
```

Per our histogram, Class A appears most frequently, with the other variables all having a strong showing in our training data set.  With these frequencies, accurate model prediction for each variable should not be difficult to achieve.  The histogram also gives us a basic idea of what we can expect to see when running our final model on the test/validation data set.

###Model Creation

Let's begin with a basic classification tree model.  We have 52 variables to consider (with column 53 being our 'classe' variable), therefore our initial model with all variables included will most likely not be our final model due to potential variable correlation.  Since we are trying to predict class type, let's start with the rpart method.

```{r, cache=TRUE}
dim(newtrain)
set.seed(777)
modFitstart <- train(classe ~ .,method="rpart",data=newtrain)
fancyRpartPlot(modFitstart$finalModel)
```

Using the rattle package, we can make a dendrogram that shows the predicted class selection based on the rpart model.  However, one big issue sticks out from this plot - given the frequencies in our histogram, how is it not possible to reach class D as an outcome using this method?  A review of model accuracy shows that the rpart method using all available variables is little better than random chance guessing.  Pretty classification trees do not indicate accuracy, and this is not the model for us.

```{r, cache=TRUE}
modFitstart
```

Let's use the caret package's preprocessing abilities to help in our model building.  All 52 variables may not be necessary - instead, we can eliminate some redundant variables that may unnecessarily complicate our model.  Only those features that impact class variability should be considered.

```{r, cache=TRUE}
preProc<-preProcess(newtrain[,-53],method="pca",thresh=0.95)
preProc
```

Per the above principal component analysis, only 26 components are needed to capture 95% of the variance.  Since we only want to use relevant features, we remove those variables that aren't useful, may lead to overfitting, or may otherwise mislead our final results.

```{r, cache=TRUE}
predtrain<-predict(preProc,newtrain[,-53])
```

Since it is accuracy we want, perhaps the best method to use is Random Forest as it is one of the best performing algorithms for prediction purposes.  The authors of the original paper** also use the random forest method in their review.  If running the below code, please be aware that this does take some computational time to run.

```{r, cache=TRUE}
set.seed(777)
modFit<-train(newtrain$classe~.,data=predtrain,method="rf",trControl=trainControl(method="cv"),importance=TRUE)
modFit$finalModel
```

Our results of the random forest method show excellent accuracy, with an OOB error rate of 3%.  Although we assume this to be slightly underestimated when applied to the testing set, we should still be within our 95% accuracy goal.

Since the training and test sets must be preprocessed in the same way for cross-validation purposes, we use the same preprocessing techniques on the test set as used on the training set.  We then run a confusion matrix to see how well our model performs on the testing data set we created in the Data Transformation section.

```{r, cache=TRUE}
predtest <- predict(preProc,newtest[,-53])
confusion<-confusionMatrix(newtest$classe,predict(modFit,predtest))
confusion
```

Our results appear to be solid, with most predictions falling squarely in the correct class.  Our accuracy rate is 96.7%, and our out of sample error rate is 1-accuracy, or 3.3%.

###Final Results

The final step of our project is to take the 20 test cases set aside as a test/validation set and submit them for automatic review.  Let's run our machine learning algorithm on the 20 test cases and see how we did:

```{r, cache=TRUE}
finalpredict<-predict(preProc,testingnoNA)
finalresults<-predict(modFit,finalpredict)
finalresults
```

```{r, cache=TRUE, eval=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(finalresults)
```

20/20 is our final result - perfect!  


**Source:  Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

